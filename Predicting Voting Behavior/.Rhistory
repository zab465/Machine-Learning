F1 <- F1_Score(y_pred = predictions_fix, y_true = test_Y_notsmote, positive = "1")
# Print F1 Score
print(paste("F1 Score on test data with formula:", F1))
detach("package:ModelMetrics", unload = TRUE)
detach("package:MLmetrics", unload = TRUE)
# Evaluate the model on the test set
predictions_fix <- predict(model_fix, newdata = test_X_notsmote)
# default threshold of 0.5 used
# because don't have any information on the meaning of the classes
confusion_matrix_fix <- confusionMatrix(predictions_fix, test_Y_notsmote, positive = "1")
f1_score_fix <- confusion_matrix_fix$byClass["F1"]
print(paste("F1 Score on test data:", f1_score_fix))
# Calculate F1 Score using MLmetrics
F1 <- F1_Score(y_pred = predictions_fix, y_true = test_Y_notsmote, positive = "1")
library(MLmetrics)
# Calculate F1 Score using MLmetrics
F1 <- F1_Score(y_pred = predictions_fix, y_true = test_Y_notsmote, positive = "1")
# Print F1 Score
print(paste("F1 Score on test data with formula:", F1))
set.seed(47)
model_fix <- randomForest(y_train_smote ~.,
data = X_train_fix,
mtry = 22,
ntree = 900)
print(model_fix)
# Evaluate the model on the test set
predictions_fix <- predict(model_fix, newdata = test_X_notsmote)
# default threshold of 0.5 used
# because don't have any information on the meaning of the classes
confusion_matrix_fix <- confusionMatrix(predictions_fix, test_Y_notsmote, positive = "0")
f1_score_fix <- confusion_matrix_fix$byClass["F1"]
print(paste("F1 Score on test data:", f1_score_fix))
library(MLmetrics)
# Calculate F1 Score using MLmetrics
F1 <- F1_Score(y_pred = predictions_fix, y_true = test_Y_notsmote, positive = "0")
# Print F1 Score
print(paste("F1 Score on test data with formula:", F1))
set.seed(47)
model_fix <- randomForest(y_train_smote ~.,
data = X_train_fix,
mtry = 22,
ntree = 900)
print(model_fix)
# Evaluate the model on the test set
predictions_fix <- predict(model_fix, newdata = test_X_notsmote)
# default threshold of 0.5 used
# because don't have any information on the meaning of the classes
confusion_matrix_fix <- confusionMatrix(predictions_fix, test_Y_notsmote, positive = "1")
f1_score_fix <- confusion_matrix_fix$byClass["F1"]
print(paste("F1 Score on test data:", f1_score_fix))
library(MLmetrics)
# Calculate F1 Score using MLmetrics
F1 <- F1_Score(y_pred = predictions_fix, y_true = test_Y_notsmote, positive = "1")
# Print F1 Score
print(paste("F1 Score on test data with formula:", F1))
set.seed(47)
model_fix <- randomForest(y_train_smote ~.,
data = X_train_smote,
mtry = 22,
ntree = 900)
print(model_fix)
# Evaluate the model on the test set
predictions_fix <- predict(model_fix, newdata = test_X_notsmote)
# default threshold of 0.5 used
# because don't have any information on the meaning of the classes
confusion_matrix_fix <- confusionMatrix(predictions_fix, test_Y_notsmote, positive = "1")
f1_score_fix <- confusion_matrix_fix$byClass["F1"]
print(paste("F1 Score on test data:", f1_score_fix))
# this chunk trains the model with only the training subset of the training data rebalanced with smote
# tests the model on the testing subset of the training data which was not rebalanced with smote
# this helps to determine if the model is doing well when trained on balanced data and tested on unbalanced data
# Set seed for reproducibility
set.seed(47)
model_1 <- randomForest(y_train_smote ~.,
data = X_train_smote,
mtry = 22,
ntree = 900)
# Evaluate the model on the test set
predictions_1 <- predict(model_1, newdata = test_X_notsmote)
# default threshold of 0.5 used
# because don't have any information on the meaning of the classes
confusion_matrix_1 <- confusionMatrix(predictions_1, test_Y_notsmote, positive = "1")
f1_score_1 <- confusion_matrix_1$byClass["F1"]
print(paste("F1 Score on test data:", f1_score_1))
# this chunk trains the model with only the training subset of the training data rebalanced with smote
# tests the model on the testing subset of the training data which was not rebalanced with smote
# this helps to determine if the model is doing well when trained on balanced data and tested on unbalanced data
# Set seed for reproducibility
set.seed(47)
model_1 <- randomForest(y_train_smote ~.,
data = X_train_smote,
mtry = 22,
ntree = 900,
maxdepth = 12)
# Evaluate the model on the test set
predictions_1 <- predict(model_1, newdata = test_X_notsmote)
# default threshold of 0.5 used
# because don't have any information on the meaning of the classes
confusion_matrix_1 <- confusionMatrix(predictions_1, test_Y_notsmote, positive = "1")
f1_score_1 <- confusion_matrix_1$byClass["F1"]
print(paste("F1 Score on test data:", f1_score_1))
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(eval = TRUE)
# packages
library(glmnet)
library(missForest)
library(caret) # for cross-validation
library(knitr)
library(kableExtra)
library(SMOTEWB)
library(tidyverse)
library(randomForest)
library(ggplot2)
library(forcats)
# Read train and test datasets
train_data <- read.csv("data/assign3_train.csv")
test_data <- read.csv("data/assign3_test.csv")
# Add indicator variable to distinguish between train and test datasets
# first need to convert the character variables into factors
train_data$x13 <- as.factor(train_data$x13)
train_data$x14 <- as.factor(train_data$x14)
train_data$x25 <- as.factor(train_data$x25)
train_data$x36 <- as.factor(train_data$x36)
train_data$x46 <- as.factor(train_data$x46)
# same for the test data
test_data$x13 <- as.factor(test_data$x13)
test_data$x14 <- as.factor(test_data$x14)
test_data$x25 <- as.factor(test_data$x25)
test_data$x36 <- as.factor(test_data$x36)
test_data$x46 <- as.factor(test_data$x46)
test_data$y <- "y"
train_data$is_train <- 1
test_data$is_train <- 0
# Combine train and test datasets
combined_data <- rbind(train_data, test_data)
# Convert factor variables to character variables temporarily
factor_vars <- sapply(combined_data, is.factor)
combined_data[factor_vars] <- lapply(combined_data[factor_vars], as.character)
# Ensure all factor variables have the same levels across train and test datasets
for (col in names(combined_data)[factor_vars]) {
levels_train <- levels(train_data[[col]])
levels_test <- levels(test_data[[col]])
levels_combined <- union(levels_train, levels_test)
combined_data[[col]] <- factor(combined_data[[col]], levels = levels_combined)
}
# Convert indicator variable back to factor
combined_data$is_train <- factor(combined_data$is_train)
# Filter out test data from combined dataset
train_combined <- combined_data[combined_data$is_train == 1, ]
# Remove the indicator variable (optional)
train_combined$is_train <- NULL
test_data$y <- NULL
# add missing value indicators to the training data
train_combined$x18_indicator <- as.numeric(is.na(train_combined$x18))
train_combined$x38_indicator <- as.numeric(is.na(train_combined$x38))
train_combined$x43_indicator <- as.numeric(is.na(train_combined$x43))
train_combined$y <- as.numeric(train_combined$y)
# repeat for the test data
test_data$x18_indicator <- as.numeric(is.na(test_data$x18))
test_data$x38_indicator <- as.numeric(is.na(test_data$x38))
test_data$x43_indicator <- as.numeric(is.na(test_data$x43))
# Identify numerical variables
numerical_vars <- c("x18", "x38", "x43")
# Impute missing values for numerical variables using mean
for (var in numerical_vars) {
train_combined[[var]][is.na(train_combined[[var]])] <- mean(train_combined[[var]], na.rm = TRUE)
}
# Identify the mode for x13
mode_x13 <- names(sort(table(train_combined$x13), decreasing = TRUE))[1]
# Impute missing values (empty levels) with mode
train_combined$x13[train_combined$x13 == ""] <- mode_x13
# train the model to fill in the missing values in the numeric columns
# keep the outcome variable, so that model can train with it as well
# try 4 different mtry values and compare the OOB NRMSE on each of these to select the appropriate number of variables selected at each stage of the tree
# keep depth of trees 10 and 100 trees in forest
# exclude y from the missforest training
data_to_impute <- train_combined[, !names(train_combined) %in% c("y")]
# Define train control parameters for cross-validation
control_params <- trainControl(method = "cv", number = 5)
# Define mtry options
mtry_grid <- expand.grid(mtry = c(2, 5, sqrt(ncol(data_to_impute)), 10))
# Train missForest models with different mtry values using cross-validation
missForest_models <- lapply(mtry_grid$mtry, function(mtry) {
missForest(data_to_impute, maxiter = 10, ntree = 100, replace = TRUE, mtry = mtry)
})
# Extract the OOBerror (NRMSE) from each run
nrmse_values <- sapply(missForest_models, function(model) model$OOBerror[1])
# Find the optimal mtry value
optimal_mtry <- mtry_grid$mtry[which.min(nrmse_values)]
# Train missForest model with the optimal mtry value
optimal_missForest_model <- missForest(data_to_impute, maxiter = 10, ntree = 100, replace = TRUE, mtry = optimal_mtry)
# Impute missing values in the original dataset using the trained model
imputed_data_train <- optimal_missForest_model$ximp
train_combined$x18 <- ifelse(is.na(train_combined$x18), imputed_data_train$x18, train_combined$x18)
train_combined$x38 <- ifelse(is.na(train_combined$x38), imputed_data_train$x38, train_combined$x38)
train_combined$x43 <- ifelse(is.na(train_combined$x43), imputed_data_train$x43, train_combined$x43)
cat_to_encode <- c("x13", "x14", "x25", "x36", "x46")
# Isolate categorical data for encoding
selected_variables_encode <- train_combined[, cat_to_encode]
# One-hot encode categorical variables
encoded_data <- model.matrix(~ . - 1, data = selected_variables_encode)
# Combine the one-hot encoded data with the rest of the train_combined dataframe, excluding the original categorical columns
train_combined <- cbind(train_combined[, !names(train_combined) %in% cat_to_encode], encoded_data)
categorical_cols <- sapply(train_combined, is.factor)
# Iterate over each categorical variable
for (col in names(train_combined)[categorical_cols]) {
# Convert factor levels to numerical labels
train_combined[[col]] <- as.numeric(factor(train_combined[[col]]))
}
# transform the outcome in a factor
train_combined$y <- factor(train_combined$y, levels = c("0", "1"))
# create a partition in the training set to apply smote on
train_index <- createDataPartition(train_combined$y, p = 0.8, list = FALSE)
test_index_smote <- setdiff(1:nrow(train_combined), train_index)  # Remaining 20% indices
X_subset <- train_combined[train_index,]
# https://stackoverflow.com/questions/67085791/package-to-do-smote-in-r
# calculate the imbalance ratio to determine how to resample
imbalance_ratio_train <- sum(X_subset$y == 0) / sum(X_subset$y == 1)
# Calculate the number of synthetic samples to generate for minority class
num_synthetic_train <- round((imbalance_ratio_train - 1) * sum(X_subset$y == 1))
# Calculate a suitable value for k
k <- min(5, num_synthetic_train)
# Perform SMOTE with appropriate k value
resampled_train <- SMOTE(X_subset[, -which(names(X_subset) == "y")],
X_subset$y,
k = k)
# Extract resampled data
balanced_df_train <- as.data.frame(resampled_train$x_new)
balanced_df_train$y <- resampled_train$y_new
X_train_smote <- balanced_df_train[, -which(names(balanced_df_train) == "y")]
# Extract "y" from balanced_df_1 as y_train
y_train_smote <- balanced_df_train$y
# Create test_X and test_Y from the remaining 20% of imputed_data
test_X_notsmote <- train_combined[test_index_smote, -which(names(train_combined) == "y")]
test_Y_notsmote <- train_combined[test_index_smote, "y"]
# After applying SMOTE, explicitate the factor levels
y_train_smote <- factor(y_train_smote, levels = c("0", "1"))
test_Y_notsmote <- factor(test_Y_notsmote, levels = c("0", "1"))
# this chunk trains the model with only the training subset of the training data rebalanced with smote
# tests the model on the testing subset of the training data which was not rebalanced with smote
# this helps to determine if the model is doing well when trained on balanced data and tested on unbalanced data
# Set seed for reproducibility
set.seed(47)
model_1 <- randomForest(y_train_smote ~.,
data = X_train_smote,
mtry = 22,
ntree = 900,
maxdepth = 12)
# Evaluate the model on the test set
predictions_1 <- predict(model_1, newdata = test_X_notsmote)
# default threshold of 0.5 used
# because don't have any information on the meaning of the classes
confusion_matrix_1 <- confusionMatrix(predictions_1, test_Y_notsmote, positive = "1")
f1_score_1 <- confusion_matrix_1$byClass["F1"]
print(paste("F1 Score on test data:", f1_score_1))
# now that model was trained with only part of the data being balanced, to obtain maximal performance on the test set
# we can rebalance the entire dataset with smote
# calculate the imbalance ratio to determine how to resample
imbalance_ratio_fulltrain <- sum(train_combined$y == 0) / sum(train_combined$y == 1)
# Calculate the number of synthetic samples to generate for minority class
num_synthetic_fulltrain <- round((imbalance_ratio_fulltrain - 1) * sum(train_combined$y == 1))
# Calculate a suitable value for k
k <- min(5, num_synthetic_fulltrain)  # Choose a smaller value of k, e.g., 5
# Perform SMOTE with appropriate k value
resampled_fulltrain <- SMOTE(train_combined[, -which(names(train_combined) == "y")],
train_combined$y,
k = k)
# Extract resampled data
balanced_df_fulltrain <- as.data.frame(resampled_fulltrain$x_new)
balanced_df_fulltrain$y <- resampled_fulltrain$y_new
# this is the final re-division of the data post-smote on the entire dataset to re-train the final model
set.seed(42)
# Create data partitions for training and testing
train_index <- createDataPartition(balanced_df_fulltrain$y, p = 0.8, list = FALSE)
X_fulltrain <- balanced_df_fulltrain[train_index, -which(names(balanced_df_fulltrain) == "y")]
y_fulltrain <- balanced_df_fulltrain[train_index, "y"]
X_fulltest <- balanced_df_fulltrain[-train_index, -which(names(balanced_df_fulltrain) == "y")]
y_fulltest <- balanced_df_fulltrain[-train_index, "y"]
# Convert response variables to factors
y_fulltrain <- as.factor(y_fulltrain)
y_fulltest <- as.factor(y_fulltest)
# Prepare the test data by filling in the missing values with missForest like in the training set
test_data_imputed <- missForest(X_fulltest, maxiter = 10, ntree = 100)
# Define train control parameters for cross-validation
control_params_test <- trainControl(method = "cv", number = 5)
# Define mtry options
mtry_grid_test <- expand.grid(mtry = c(2, 5, sqrt(ncol(test_data_imputed$ximp)), 10))
# Train missForest models with different mtry values using cross-validation
missForest_models_test <- lapply(mtry_grid_test$mtry, function(mtry) {
missForest(test_data_imputed$ximp, maxiter = 10, ntree = 100, replace = TRUE, mtry = mtry)
})
# Extract the OOBerror (NRMSE) from each run
nrmse_values_test <- sapply(missForest_models_test, function(model) model$OOBerror[1])
# Find the optimal mtry value
optimal_mtry_test <- mtry_grid_test$mtry[which.min(nrmse_values_test)]
# Train missForest model with the optimal mtry value
optimal_missForest_model_test <- missForest(X_fulltest, maxiter = 10, ntree = 100, replace = TRUE, mtry = optimal_mtry_test)
# Impute missing values in the original dataset using the trained model
imputed_data_test <- optimal_missForest_model_test$ximp
# Replace missing values in specific columns of test_data with imputed values
test_data$x18 <- ifelse(is.na(test_data$x18), imputed_data_test$x18, test_data$x18)
test_data$x38 <- ifelse(is.na(test_data$x38), imputed_data_test$x38, test_data$x38)
test_data$x43 <- ifelse(is.na(test_data$x43), imputed_data_test$x43, test_data$x43)
# here I train the model on the entire train dataset where I used smote
# Set seed for reproducibility
set.seed(42)
rf_fulltrain <- randomForest(y_train_smote ~.,
data = X_train_smote,
mtry = 22,
ntree = 900, maxdepth = 12)
# Evaluate the model on the test set
unseen_predictions <- predict(rf_fulltrain, newdata = X_fulltest)
predictions_df <- data.frame(y = unseen_predictions)
# Write the data frame to a CSV file
write.csv(predictions_df, file = "31696.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(eval = TRUE)
# packages
library(glmnet)
library(missForest)
library(caret) # for cross-validation
library(knitr)
library(kableExtra)
library(SMOTEWB)
library(tidyverse)
library(randomForest)
library(ggplot2)
library(forcats)
# Read train and test datasets
train_data <- read.csv("data/assign3_train.csv")
test_data <- read.csv("data/assign3_test.csv")
# Add indicator variable to distinguish between train and test datasets
# first need to convert the character variables into factors
train_data$x13 <- as.factor(train_data$x13)
train_data$x14 <- as.factor(train_data$x14)
train_data$x25 <- as.factor(train_data$x25)
train_data$x36 <- as.factor(train_data$x36)
train_data$x46 <- as.factor(train_data$x46)
# same for the test data
test_data$x13 <- as.factor(test_data$x13)
test_data$x14 <- as.factor(test_data$x14)
test_data$x25 <- as.factor(test_data$x25)
test_data$x36 <- as.factor(test_data$x36)
test_data$x46 <- as.factor(test_data$x46)
test_data$y <- "y"
train_data$is_train <- 1
test_data$is_train <- 0
# Combine train and test datasets
combined_data <- rbind(train_data, test_data)
# Convert factor variables to character variables temporarily
factor_vars <- sapply(combined_data, is.factor)
combined_data[factor_vars] <- lapply(combined_data[factor_vars], as.character)
# Ensure all factor variables have the same levels across train and test datasets
for (col in names(combined_data)[factor_vars]) {
levels_train <- levels(train_data[[col]])
levels_test <- levels(test_data[[col]])
levels_combined <- union(levels_train, levels_test)
combined_data[[col]] <- factor(combined_data[[col]], levels = levels_combined)
}
# Convert indicator variable back to factor
combined_data$is_train <- factor(combined_data$is_train)
# Filter out test data from combined dataset
train_combined <- combined_data[combined_data$is_train == 1, ]
# Remove the indicator variable (optional)
train_combined$is_train <- NULL
test_data$y <- NULL
# add missing value indicators to the training data
train_combined$x18_indicator <- as.numeric(is.na(train_combined$x18))
train_combined$x38_indicator <- as.numeric(is.na(train_combined$x38))
train_combined$x43_indicator <- as.numeric(is.na(train_combined$x43))
train_combined$y <- as.numeric(train_combined$y)
# repeat for the test data
test_data$x18_indicator <- as.numeric(is.na(test_data$x18))
test_data$x38_indicator <- as.numeric(is.na(test_data$x38))
test_data$x43_indicator <- as.numeric(is.na(test_data$x43))
# train the model to fill in the missing values in the numeric columns
# keep the outcome variable, so that model can train with it as well
# try 4 different mtry values and compare the OOB NRMSE on each of these to select the appropriate number of variables selected at each stage of the tree
# keep depth of trees 10 and 100 trees in forest
# exclude y from the missforest training
data_to_impute <- train_combined[, !names(train_combined) %in% c("y")]
# Define train control parameters for cross-validation
control_params <- trainControl(method = "cv", number = 5)
# Define mtry options
mtry_grid <- expand.grid(mtry = c(2, 5, sqrt(ncol(data_to_impute)), 10))
# Train missForest models with different mtry values using cross-validation
missForest_models <- lapply(mtry_grid$mtry, function(mtry) {
missForest(data_to_impute, maxiter = 10, ntree = 100, replace = TRUE, mtry = mtry)
})
# Extract the OOBerror (NRMSE) from each run
nrmse_values <- sapply(missForest_models, function(model) model$OOBerror[1])
# Find the optimal mtry value
optimal_mtry <- mtry_grid$mtry[which.min(nrmse_values)]
# Train missForest model with the optimal mtry value
optimal_missForest_model <- missForest(data_to_impute, maxiter = 10, ntree = 100, replace = TRUE, mtry = optimal_mtry)
# Impute missing values in the original dataset using the trained model
imputed_data_train <- optimal_missForest_model$ximp
train_combined$x18 <- ifelse(is.na(train_combined$x18), imputed_data_train$x18, train_combined$x18)
train_combined$x38 <- ifelse(is.na(train_combined$x38), imputed_data_train$x38, train_combined$x38)
train_combined$x43 <- ifelse(is.na(train_combined$x43), imputed_data_train$x43, train_combined$x43)
categorical_cols <- sapply(train_combined, is.factor)
# Iterate over each categorical variable
for (col in names(train_combined)[categorical_cols]) {
# Convert factor levels to numerical labels
train_combined[[col]] <- as.numeric(factor(train_combined[[col]]))
}
# transform the outcome in a factor
train_combined$y <- factor(train_combined$y, levels = c("0", "1"))
# create a partition in the training set to apply smote on
train_index <- createDataPartition(train_combined$y, p = 0.8, list = FALSE)
test_index_smote <- setdiff(1:nrow(train_combined), train_index)  # Remaining 20% indices
X_subset <- train_combined[train_index,]
# https://stackoverflow.com/questions/67085791/package-to-do-smote-in-r
# calculate the imbalance ratio to determine how to resample
imbalance_ratio_train <- sum(X_subset$y == 0) / sum(X_subset$y == 1)
# Calculate the number of synthetic samples to generate for minority class
num_synthetic_train <- round((imbalance_ratio_train - 1) * sum(X_subset$y == 1))
# Calculate a suitable value for k
k <- min(5, num_synthetic_train)
# Perform SMOTE with appropriate k value
resampled_train <- SMOTE(X_subset[, -which(names(X_subset) == "y")],
X_subset$y,
k = k)
# Extract resampled data
balanced_df_train <- as.data.frame(resampled_train$x_new)
balanced_df_train$y <- resampled_train$y_new
X_train_smote <- balanced_df_train[, -which(names(balanced_df_train) == "y")]
# Extract "y" from balanced_df_1 as y_train
y_train_smote <- balanced_df_train$y
# Create test_X and test_Y from the remaining 20% of imputed_data
test_X_notsmote <- train_combined[test_index_smote, -which(names(train_combined) == "y")]
test_Y_notsmote <- train_combined[test_index_smote, "y"]
# After applying SMOTE, explicitate the factor levels
y_train_smote <- factor(y_train_smote, levels = c("0", "1"))
test_Y_notsmote <- factor(test_Y_notsmote, levels = c("0", "1"))
# this chunk trains the model with only the training subset of the training data rebalanced with smote
# tests the model on the testing subset of the training data which was not rebalanced with smote
# this helps to determine if the model is doing well when trained on balanced data and tested on unbalanced data
# Set seed for reproducibility
set.seed(47)
model_1 <- randomForest(y_train_smote ~.,
data = X_train_smote,
mtry = 22,
ntree = 900,
maxdepth = 12)
# Evaluate the model on the test set
predictions_1 <- predict(model_1, newdata = test_X_notsmote)
# default threshold of 0.5 used
# because don't have any information on the meaning of the classes
confusion_matrix_1 <- confusionMatrix(predictions_1, test_Y_notsmote, positive = "1")
f1_score_1 <- confusion_matrix_1$byClass["F1"]
print(paste("F1 Score on test data:", f1_score_1))
# now that model was trained with only part of the data being balanced, to obtain maximal performance on the test set
# we can rebalance the entire dataset with smote
# calculate the imbalance ratio to determine how to resample
imbalance_ratio_fulltrain <- sum(train_combined$y == 0) / sum(train_combined$y == 1)
# Calculate the number of synthetic samples to generate for minority class
num_synthetic_fulltrain <- round((imbalance_ratio_fulltrain - 1) * sum(train_combined$y == 1))
# Calculate a suitable value for k
k <- min(5, num_synthetic_fulltrain)  # Choose a smaller value of k, e.g., 5
# Perform SMOTE with appropriate k value
resampled_fulltrain <- SMOTE(train_combined[, -which(names(train_combined) == "y")],
train_combined$y,
k = k)
# Extract resampled data
balanced_df_fulltrain <- as.data.frame(resampled_fulltrain$x_new)
balanced_df_fulltrain$y <- resampled_fulltrain$y_new
# this is the final re-division of the data post-smote on the entire dataset to re-train the final model
set.seed(42)
# Create data partitions for training and testing
train_index <- createDataPartition(balanced_df_fulltrain$y, p = 0.8, list = FALSE)
X_fulltrain <- balanced_df_fulltrain[train_index, -which(names(balanced_df_fulltrain) == "y")]
y_fulltrain <- balanced_df_fulltrain[train_index, "y"]
X_fulltest <- balanced_df_fulltrain[-train_index, -which(names(balanced_df_fulltrain) == "y")]
y_fulltest <- balanced_df_fulltrain[-train_index, "y"]
# Convert response variables to factors
y_fulltrain <- as.factor(y_fulltrain)
y_fulltest <- as.factor(y_fulltest)
# Prepare the test data by filling in the missing values with missForest like in the training set
test_data_imputed <- missForest(X_fulltest, maxiter = 10, ntree = 100)
# Define train control parameters for cross-validation
control_params_test <- trainControl(method = "cv", number = 5)
# Define mtry options
mtry_grid_test <- expand.grid(mtry = c(2, 5, sqrt(ncol(test_data_imputed$ximp)), 10))
# Train missForest models with different mtry values using cross-validation
missForest_models_test <- lapply(mtry_grid_test$mtry, function(mtry) {
missForest(test_data_imputed$ximp, maxiter = 10, ntree = 100, replace = TRUE, mtry = mtry)
})
# Extract the OOBerror (NRMSE) from each run
nrmse_values_test <- sapply(missForest_models_test, function(model) model$OOBerror[1])
# Find the optimal mtry value
optimal_mtry_test <- mtry_grid_test$mtry[which.min(nrmse_values_test)]
# Train missForest model with the optimal mtry value
optimal_missForest_model_test <- missForest(X_fulltest, maxiter = 10, ntree = 100, replace = TRUE, mtry = optimal_mtry_test)
# Impute missing values in the original dataset using the trained model
imputed_data_test <- optimal_missForest_model_test$ximp
# Replace missing values in specific columns of test_data with imputed values
test_data$x18 <- ifelse(is.na(test_data$x18), imputed_data_test$x18, test_data$x18)
test_data$x38 <- ifelse(is.na(test_data$x38), imputed_data_test$x38, test_data$x38)
test_data$x43 <- ifelse(is.na(test_data$x43), imputed_data_test$x43, test_data$x43)
# here I train the model on the entire train dataset where I used smote
# Set seed for reproducibility
set.seed(42)
rf_fulltrain <- randomForest(y_train_smote ~.,
data = X_train_smote,
mtry = 22,
ntree = 900, maxdepth = 12)
# Evaluate the model on the test set
unseen_predictions <- predict(rf_fulltrain, newdata = X_fulltest)
predictions_df <- data.frame(y = unseen_predictions)
# Write the data frame to a CSV file
write.csv(predictions_df, file = "31696.csv", row.names = FALSE)
